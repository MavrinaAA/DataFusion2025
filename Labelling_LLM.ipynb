{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from src.category_tree.category_tree import CategoryTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "CAT_ID_COL = \"cat_id\"\n",
    "CAT_NAME_COL = 'cat_name'\n",
    "TITLE_COL = \"source_name\"\n",
    "PART_TYPE_COL = \"part_type\"\n",
    "PART_COL = \"part\"\n",
    "HASH_ID_COL = 'hash_id'\n",
    "\n",
    "CAT_PATH = \"data/raw/category_tree.csv\"\n",
    "BAD_LABELED_PATH = 'data/processed/bad_labeled.parquet'\n",
    "SAVE_FILE_NAME = 'data/processed/bad_labeled_qwen2.5:3b.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tree = CategoryTree(category_tree_path=CAT_PATH)\n",
    "categor = pd.read_csv(CAT_PATH)\n",
    "\n",
    "# Load dataset with mislabeled samples\n",
    "df = pd.read_parquet(BAD_LABELED_PATH, columns=[TITLE_COL, CAT_ID_COL, HASH_ID_COL])\n",
    "\n",
    "# Filter dataset to only include samples from \"pre-leaf\" categories (one level above leaf nodes)\n",
    "pre_leaf_nodes = set(categor[categor.cat_id.isin(category_tree.leaf_nodes)].parent_id.to_list())\n",
    "df = df[df.cat_id.isin(pre_leaf_nodes)]\n",
    "df = df.sample(frac=1,random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Enrich dataset with category names by merging with category metadata\n",
    "df = df.merge(categor, on='cat_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4181, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to load previously saved progress (if exists)\n",
    "try:\n",
    "    df_save = pd.read_csv(SAVE_FILE_NAME)  # Load saved DataFrame\n",
    "except FileNotFoundError:\n",
    "    # If no save file found, init an empty DataFrame with expected columns\n",
    "    df_save = pd.DataFrame(columns=[HASH_ID_COL, TITLE_COL, 'pred_cat_id'])\n",
    "df_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:59<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ollama Settings\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434/api/chat\"\n",
    "MODEL_NAME = \"qwen2.5:3b\"   # Model name as loaded in Ollama\n",
    "\n",
    "def ask_ollama(prompt, history=[]):\n",
    "    \"\"\"\n",
    "    Sends a request to the Ollama API with a given prompt and conversation history.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.2,\n",
    "            \"stream\": False\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error requesting Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "# Data Preparation\n",
    "start_categor = categor[categor['parent_id'].isna()][CAT_NAME_COL].tolist()\n",
    "categor_dict = categor.groupby('parent_id')[CAT_NAME_COL].apply(list).to_dict()\n",
    "\n",
    "# Extract necessary columns from the dataframe into lists\n",
    "source_name_list = df[TITLE_COL].tolist()\n",
    "hash_id_list = df[HASH_ID_COL].tolist()\n",
    "cat_name_list = df[CAT_NAME_COL].tolist()\n",
    "cat_id_list = df[CAT_ID_COL].tolist()\n",
    "\n",
    "# List to store predicted category IDs\n",
    "cat_id_pred = []\n",
    "\n",
    "# Determine the starting index for processing (continuing from previously saved data)\n",
    "start_index = df_save.shape[0]\n",
    "count = 0  # Counter for tracking processed items\n",
    "\n",
    "for name, hash_id, cat_name, cat_id in tqdm(\n",
    "    zip(source_name_list[start_index:], hash_id_list[start_index:], cat_name_list[start_index:], cat_id_list[start_index:]), \n",
    "    total=len(source_name_list[start_index:])\n",
    "):\n",
    "    count += 1\n",
    "\n",
    "    # Get subcategories for the current category ID\n",
    "    subcategories = categor_dict.get(cat_id, [])\n",
    "\n",
    "    # Format the subcategory list as a string\n",
    "    lst = \"\\n - \" + \"\\n - \".join(subcategories)\n",
    "    cat = cat_name # Current category name\n",
    "\n",
    "    # Initialize the conversation history with system instructions\n",
    "    chat_history = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"Ты полезный помощник по определению категории товара и списка категорий, который строго следует инструкции. \"\n",
    "            \"Ты всегда выбираешь только одну категорию из предложенного списка, без лишних слов.\"\n",
    "        )\n",
    "    }]\n",
    "\n",
    "    prompt = (\n",
    "        f\"Товар: '{name}'.\\n\"\n",
    "        f\"Текущая категория: {cat}.\\n\"\n",
    "        f\"Выбери наиболее подходящую подкатегорию из списка:\\n{lst}\\n\"\n",
    "        f\"Ответ должен быть строго только названием категории из списка, без лишних слов.\"\n",
    "    )\n",
    "\n",
    "    # print('prompt: \\n', prompt)\n",
    "    # print('cat_id: \\n', cat_id)\n",
    "\n",
    "    answer = ask_ollama(prompt, chat_history)\n",
    "    if not answer:\n",
    "        # If Ollama request fails, append failure result and break the loop\n",
    "        cat_id_pred.append([hash_id, name, None])\n",
    "        break\n",
    "\n",
    "    # Clean the response\n",
    "    answer = answer.strip().strip('.')\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    # Find the subcategory ID based on Ollama's response\n",
    "    mask = (categor[CAT_NAME_COL] == answer) & (categor['parent_id'] == cat_id)\n",
    "    category = categor.loc[mask]\n",
    "\n",
    "    if category.empty:\n",
    "        # print(\"Category not found\")\n",
    "        cat_id_pred.append([hash_id, name, np.nan])\n",
    "    else:\n",
    "        cat_id_pred.append([hash_id, name, answer])\n",
    "\n",
    "    # Saving\n",
    "    if count % 10 == 0:\n",
    "        df_save = pd.concat([df_save, pd.DataFrame(cat_id_pred, columns=[HASH_ID_COL, TITLE_COL, 'pred_cat_id'])])\n",
    "        df_save.to_csv(SAVE_FILE_NAME, index=False, na_rep='NaN')\n",
    "        cat_id_pred = []\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(df_save.shape)\n",
    "\n",
    "# Save any remaining predictions\n",
    "if cat_id_pred:\n",
    "    df_save = pd.concat([df_save, pd.DataFrame(cat_id_pred, columns=[HASH_ID_COL, TITLE_COL, 'pred_cat_id'])])\n",
    "    df_save.to_csv(SAVE_FILE_NAME, index=False, na_rep='NaN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
